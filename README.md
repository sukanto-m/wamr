# WhoAteMyRAM (wamr)

[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![Platform](https://img.shields.io/badge/platform-Linux%20%7C%20macOS-lightgrey)](https://github.com/yourusername/whoatemyram)

**LLM-powered memory analysis for developers**

Stop guessing what's eating your RAM. Get intelligent, actionable insights in plain English.

![Demo](demo.png)

---

## ğŸ¯ The Problem

You're coding. System slows down. You check `htop` and see 87% memory usage.

**Now what?**

- Which processes are safe to kill?
- What's a memory leak vs. normal behavior?
- Should you close those 47 Chrome tabs?

Traditional tools (`htop`, `free`, `top`) show you **data**. WhoAteMyRAM gives you **understanding**.

---

## âœ¨ The Solution

```bash
$ wamr

ğŸ’¾ WhoAteMyRAM - Memory Analysis
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸŸ¡ WARNING - 73.2% used (11.7 GB / 16.0 GB)

ğŸ“Š Summary: High memory usage detected with multiple browser 
processes and development tools running.

ğŸ”´ HIGH PRIORITY (2 issues)

â€¢ chrome (PID 12091) - 3.2 GB
  Reason: 47 tabs open, 28 idle for >2 hours
  Action: Close idle tabs to reclaim ~1.8 GB
  ğŸ’» Command: chrome://discards

â€¢ docker-compose (PID 8432) - 2.1 GB
  Reason: old-project containers haven't been accessed in 14 days
  Action: Stop unused containers
  ğŸ’» Command: cd ~/old-project && docker-compose down

ğŸŸ¢ SAFE TO IGNORE (3 processes)

â€¢ node (1.4 GB) - Active webpack dev server
â€¢ ollama (1.1 GB) - Running this analysis
â€¢ slack (856 MB) - Active messaging client

ğŸ’° Total Reclaimable: ~3.9 GB
```

---

## ğŸš€ Quick Start

### Prerequisites

- Python 3.8+
- [Ollama](https://ollama.ai)
- Linux or macOS

### Install

```bash
# 1. Clone the repo
git clone https://github.com/yourusername/whoatemyram.git
cd whoatemyram

# 2. Install dependencies
pip install -r requirements.txt

# 3. Install Ollama and pull a model
curl -fsSL https://ollama.ai/install.sh | sh
ollama serve &
ollama pull llama3.2:3b

# 4. Run it!
python3 wamr.py
```

Or use the installer:

```bash
chmod +x install.sh
./install.sh
```

---

## ğŸ® Usage

```bash
# Full LLM analysis
wamr

# See demo output (no Ollama needed)
wamr --demo

# Quick check (no LLM)
wamr --no-llm

# Use different model
wamr --model llama3.1:8b

# Output as JSON
wamr --json

# See all options
wamr --help
```

---

## ğŸŒŸ Features

- ğŸ¤– **LLM-powered analysis** - Uses local Ollama for intelligent insights
- ğŸ¯ **Actionable suggestions** - Get specific commands to fix issues
- ğŸ”’ **Privacy-first** - Everything runs locally, no cloud APIs
- âš¡ **Fast** - Analysis completes in 5-15 seconds
- ğŸ¨ **Beautiful output** - Color-coded, emoji-enhanced terminal UI
- ğŸ”§ **Zero config** - Works out of the box
- ğŸ **Cross-platform** - Linux and macOS support
- ğŸ“Š **Multiple modes** - Demo, quick check, or full analysis

---

## ğŸ“– Documentation

- **[QUICKSTART.md](QUICKSTART.md)** - 5-minute setup guide
- **[EXAMPLES.md](EXAMPLES.md)** - Usage scenarios and examples
- **[MACOS.md](MACOS.md)** - macOS-specific setup ğŸ
- **[CONTRIBUTING.md](CONTRIBUTING.md)** - How to contribute
- **[HOWTO.md](HOWTO.md)** - Detailed local setup

---

## ğŸ”§ How It Works

1. **Scan** - Reads system memory and process data
2. **Analyze** - Sends data to local LLM (via Ollama)
3. **Understand** - LLM identifies issues, patterns, and opportunities
4. **Present** - Shows color-coded, actionable results

```
System Data â†’ Local LLM â†’ Intelligent Analysis â†’ You Fix Issues
```

**No cloud. No tracking. Just smart, local analysis.**

---

## ğŸ’¡ Why Local LLM?

| Cloud Tools | WhoAteMyRAM |
|-------------|-------------|
| ğŸ’° Monthly fees | âœ… Free forever |
| â˜ï¸ Send data to cloud | âœ… 100% local |
| ğŸŒ Requires internet | âœ… Works offline |
| ğŸ“Š Generic advice | âœ… Context-aware |
| ğŸ”’ Privacy concerns | âœ… Privacy-first |

---

## ğŸ¯ Use Cases

### For Developers
- Debug memory issues during development
- Understand what dev tools are using
- Identify memory leaks in your code

### For DevOps
- Quick diagnosis on production boxes
- Understand memory pressure
- Get actionable remediation steps

### For Everyone
- Know when to close browser tabs
- Understand what's really using memory
- Make informed decisions about killing processes

---

## ğŸ†š Comparison

| Tool | Shows | Analysis | Output |
|------|-------|----------|--------|
| `htop` | Raw process list | None | Numbers |
| `free` | Memory stats | None | Numbers |
| `top` | System stats | None | Numbers |
| Activity Monitor | Process list | None | GUI |
| **wamr** | Memory diagnosis | LLM-powered | Natural language + commands |

---

## ğŸ—ï¸ Platform Support

### Linux âœ…
- Ubuntu 20.04+
- Debian 10+
- Arch Linux
- Fedora 35+
- Any distro with `/proc/meminfo`

### macOS âœ…
- macOS 13+ (Ventura)
- macOS 14+ (Sonoma)
- macOS 15+ (Sequoia)
- Both Intel and Apple Silicon

### Windows â³
- Planned for future release

---

## ğŸ“Š Performance

- **Scan time:** <1 second
- **LLM analysis:** 3-10 seconds (model dependent)
- **Total time:** ~5-15 seconds end-to-end

**Tested models:**
- `llama3.2:1b` - Fastest (~3s)
- `llama3.2:3b` - Recommended (~5s)
- `llama3.1:8b` - Best quality (~10s)

---

## ğŸ¤ Contributing

We love contributions! See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

**Quick ways to help:**
- ğŸ› Report bugs
- ğŸ’¡ Suggest features
- ğŸ“ Improve documentation
- ğŸ”§ Submit pull requests
- â­ Star the repo!

---

## ğŸ—ºï¸ Roadmap

- [x] Linux support
- [x] macOS support
- [x] Demo mode
- [x] JSON output
- [ ] Memory leak tracking
- [ ] Historical comparison
- [ ] Interactive kill mode
- [ ] Windows support
- [ ] Homebrew formula
- [ ] More LLM backends

---

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) for details.

---

## ğŸ™ Acknowledgments

- Inspired by [witr](https://github.com/pranshuparmar/witr)
- Built with [Ollama](https://ollama.ai) and assistance from Claude AI
- Powered by local LLMs

---

## ğŸ“ Learn More

**Blog posts:**
- [Coming soon] How WhoAteMyRAM works

**Videos:**
- [Coming soon] Demo and tutorial

**Community:**
- GitHub Discussions
- Issues and PRs welcome

---

## â­ Show Your Support

If WhoAteMyRAM helps you understand your system better, give it a star! â­

It helps others discover the project.

---

## ğŸ“ Contact

- **Issues:** [GitHub Issues](https://github.com/sukanto-m/whoatemyram/issues)
- **Discussions:** [GitHub Discussions](https://github.com/sukanto-m/whoatemyram/discussions)

---

<p align="center">
  <strong>Built with â¤ï¸ for the local LLM community</strong>
  <br>
  <sub>Making system utilities intelligent, one tool at a time</sub>
</p>

<p align="center">
  <a href="#quick-start">Get Started</a> â€¢
  <a href="QUICKSTART.md">Quickstart</a> â€¢
  <a href="EXAMPLES.md">Examples</a> â€¢
  <a href="CONTRIBUTING.md">Contribute</a>
</p>
